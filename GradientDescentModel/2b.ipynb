{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMkoFQNQKWbh"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "\n",
        "class GradientDescentModel:\n",
        "\n",
        "    w = None\n",
        "    b = None\n",
        "\n",
        "    def __init__(self, data, learning_rate, nb_features, nb_epochs, ping, lmbd):\n",
        "        self.nb_features = nb_features\n",
        "        self.data = copy.deepcopy(training_data)\n",
        "        self.data['x'] = GradientDescentModel.create_feature_matrix(data['x'], self.nb_features)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.adam = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        self.nb_epochs = nb_epochs\n",
        "        self.ping = ping\n",
        "        self.avg_loss = 0\n",
        "        self.lmbd = lmbd\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def create_feature_matrix(x, nb_features):\n",
        "        tmp_features = []\n",
        "        for deg in range(1, nb_features+1):\n",
        "            tmp_features.append(np.power(x, deg))\n",
        "\n",
        "        return np.column_stack(tmp_features)\n",
        "\n",
        "\n",
        "    def test_model(self, xs=np.linspace(-2, 2, 100, dtype='float32')):\n",
        "      xs_features = GradientDescentModel.create_feature_matrix(xs, self.nb_features);\n",
        "      return xs, self.pred(xs_features)\n",
        "\n",
        "\n",
        "    def train_model(self):\n",
        "      self.w = tf.Variable(tf.zeros(self.nb_features))\n",
        "      self.b = tf.Variable(0.0)\n",
        "\n",
        "      total_loss = 0\n",
        "\n",
        "      samples_num = self.data['x'].shape[0]\n",
        "\n",
        "      for epoch in range(self.nb_epochs):\n",
        "\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for sample in range(samples_num):\n",
        "          x = self.data['x'][sample].reshape((1, self.nb_features))\n",
        "          y = self.data['y'][sample]\n",
        "\n",
        "          curr_loss = self.train_step(x, y)\n",
        "          epoch_loss += curr_loss\n",
        "\n",
        "        epoch_loss /= samples_num\n",
        "        total_loss += epoch_loss\n",
        "\n",
        "        if (epoch + 1) % self.ping == 0:\n",
        "            print(f'Model with polynomial degree {self.nb_features} and lambda {self.lmbd} | Epoch: {epoch+1}/{self.nb_epochs}| Average loss: {epoch_loss:.7f}')\n",
        "\n",
        "      self.avg_loss = total_loss/nb_epochs\n",
        "\n",
        "      return\n",
        "\n",
        "\n",
        "    def train_step(self, x, y):\n",
        "        w_grad, b_grad, loss = self.calc_grad(x, y)\n",
        "\n",
        "\n",
        "        self.adam.apply_gradients(zip([w_grad, b_grad], [self.w, self.b]))\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def calc_grad(self, x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss_val = self.loss(x, y)\n",
        "\n",
        "        w_grad, b_grad = tape.gradient(loss_val, [self.w, self.b])\n",
        "\n",
        "        return w_grad, b_grad, loss_val\n",
        "\n",
        "\n",
        "    def pred(self, x):\n",
        "        w_col = tf.reshape(self.w, (self.nb_features, 1))\n",
        "        hyp = tf.add(tf.matmul(x, w_col), self.b)\n",
        "\n",
        "        return hyp\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        prediction = self.pred(x)\n",
        "        y_col = tf.reshape(y, (-1, 1))\n",
        "        mse = tf.reduce_mean(tf.square(prediction - y_col))\n",
        "        reg = self.lmbd * tf.reduce_mean(tf.square(self.w))\n",
        "\n",
        "        return tf.add(mse, reg)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------- #\n",
        "tadija_path = '/content/drive/MyDrive/ML2024_D1/bottle.csv'\n",
        "mina_path = '/content/drive/MyDrive/6003 ML/data/bottle.csv'\n",
        "# --------------------------------------------------------- #\n",
        "\n",
        "\n",
        "training_data = dict()\n",
        "nb_features = 4\n",
        "load_rows = 300\n",
        "samples_num = 200\n",
        "\n",
        "training_data['x'], training_data['y'] = np.genfromtxt(tadija_path, dtype='float32', delimiter=',',\n",
        "                                                       skip_header=1, usecols=(5, 6), unpack=True, max_rows=load_rows)\n",
        "\n",
        "mask = np.isnan(training_data['x']) | np.isnan(training_data['y'])\n",
        "\n",
        "filtered_x = training_data['x'][~mask]\n",
        "filtered_y = training_data['y'][~mask]\n",
        "\n",
        "training_data['x'] = filtered_x[:samples_num]\n",
        "training_data['y'] = filtered_y[:samples_num]\n",
        "\n",
        "# shuffling data\n",
        "indices = np.random.permutation(samples_num)\n",
        "training_data['x'] = training_data['x'][indices]\n",
        "training_data['y'] = training_data['y'][indices]\n",
        "\n",
        "# normalization\n",
        "training_data['x'] = (training_data['x'] - np.mean(training_data['x'])) / np.std(training_data['x'])\n",
        "training_data['y'] = (training_data['y'] - np.mean(training_data['y'])) / np.std(training_data['y'])\n",
        "\n",
        "\n",
        "nb_epochs = 100\n",
        "losses = []\n",
        "lmbd = [0, 0.001, 0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(training_data['x'], training_data['y'], color='black', label='Data')\n",
        "plt.xlabel(\"Salinity of Water\")\n",
        "plt.ylabel(\"Temperature of Water (Â°C)\")\n",
        "\n",
        "\n",
        "for l in lmbd:\n",
        "\n",
        "  model = GradientDescentModel(data=training_data, learning_rate=0.001, nb_features = nb_features, nb_epochs=nb_epochs, ping=20, lmbd = l)\n",
        "  model.train_model()\n",
        "  loss_val = model.loss(model.data['x'], model.data['y']).numpy()\n",
        "  losses.append(loss_val)\n",
        "  xs, ys = model.test_model(xs=np.linspace(-2, 2, 100, dtype='float32'))\n",
        "  color = (np.random.rand(), np.random.rand(),np.random.rand())\n",
        "  plt.plot(xs, ys.numpy().tolist(), color=color, label = str(model.lmbd) + ' lambda')\n",
        "\n",
        "  print('-----')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(np.arange(len(lmbd)), losses, tick_label=[str(l) for l in lmbd])\n",
        "plt.xlabel('lambda parameter')\n",
        "plt.ylabel('Loss Function')\n",
        "plt.title('Dependency of Final Loss Function on Entire Set based on parameter lambda')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Za male lambda vrednosti, visoki koeficijenti su 'kaznjeni' relativno slabo, i samim tim funkcija troska je konzistenta ukoliko je lambda vrednosti : 0, 0.001, 0.01, 0.1, 1. Iz ovoga vidimo da L2 regularizacija ne utice preterano na ponasanje modela.\n",
        "Kada su vrednosti lambda 10 i 100, medjutim, dolazi do znacajnog rasta u vrednosti funkcije troska. Kaada je stroza 'kazna', model ima znacaja da drzi koeficijente jako malim kako bi se trosak minimizovao. Zbog ovoga fitovanje modela ispasta, jer je akcenat na odrzavanju malih koeficijenata. Ovo moze dovesti do underfittinga, samim tim uspeh modela znacajno opada zbog preterane regularizacije.\n",
        "\n"
      ],
      "metadata": {
        "id": "e2LUeyhCKX72"
      }
    }
  ]
}