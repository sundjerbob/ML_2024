{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sundjerbob/ML_2024/blob/main/MultinomialNaiveBayes/4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lC_jYW7KxG7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import csv\n",
        "import html\n",
        "import math\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import LancasterStemmer\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "class MultinomialNaiveBayes:\n",
        "\n",
        "  def __init__(self, nb_classes, nb_words, pseudocount):\n",
        "\n",
        "    self.nb_classes = nb_classes\n",
        "    self.nb_words = nb_words\n",
        "    self.pseudocount = pseudocount\n",
        "\n",
        "  def fit(self, X, Y):\n",
        "    nb_examples = X.shape[0]\n",
        "\n",
        "    self.priors = np.bincount(Y) / nb_examples\n",
        "\n",
        "    print('\\nPriors:')\n",
        "    print(self.priors)\n",
        "\n",
        "    occs = np.zeros((self.nb_classes, self.nb_words))\n",
        "    for i in range(nb_examples):\n",
        "      c = Y[i]\n",
        "      for w in range(self.nb_words):\n",
        "        cnt = X[i][w]\n",
        "        occs[c][w] += cnt\n",
        "\n",
        "    print('\\nOccurences:')\n",
        "    print(occs)\n",
        "\n",
        "    self.like = np.zeros((self.nb_classes, self.nb_words))\n",
        "    for c in range(self.nb_classes):\n",
        "      for w in range(self.nb_words):\n",
        "        up = occs[c][w] + self.pseudocount\n",
        "        down = np.sum(occs[c]) + self.nb_words*self.pseudocount\n",
        "        if down == 0:\n",
        "          self.like[c][w] = 0\n",
        "        else:\n",
        "          self.like[c][w] = up / down\n",
        "\n",
        "    print('\\nLikelihoods:')\n",
        "    print(self.like)\n",
        "    print()\n",
        "\n",
        "  def predict(self, bow):\n",
        "    # Laplace smoothing: add pseudocount to priors and likelihoods\n",
        "    smooth_priors = self.priors + 1e-10  # Adding a small epsilon to avoid division by zero\n",
        "    smooth_likelihoods = self.like + 1e-10  # Adding a small epsilon to avoid division by zero\n",
        "\n",
        "    probs = np.zeros(self.nb_classes)\n",
        "    for c in range(self.nb_classes):\n",
        "        prob = np.log(smooth_priors[c])\n",
        "        for w in range(self.nb_words):\n",
        "            cnt = bow[w]\n",
        "            prob += cnt * np.log(smooth_likelihoods[c][w])\n",
        "        probs[c] = prob\n",
        "\n",
        "    prediction = np.argmax(probs)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "  def predict_multiply(self, bow):\n",
        "    probs = np.zeros(self.nb_classes)\n",
        "    for c in range(self.nb_classes):\n",
        "      prob = self.priors[c]\n",
        "      for w in range(self.nb_words):\n",
        "        cnt = bow[w]\n",
        "        prob *= self.like[c][w] ** cnt\n",
        "      probs[c] = prob\n",
        "\n",
        "    print(probs)\n",
        "    prediction = np.argmax(probs)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "regex_url = r'https?://\\S+'\n",
        "regex_dots = r'[.:;]'\n",
        "regex_currencies= r'\\b(?:\\$|€|¥|£)\\S+'\n",
        "regex_spec_chars = r'[^a-zA-Z0-9\\s]'\n",
        "regex_numeric = r'\\b\\d+\\b|\\b\\d+/\\d+/\\d+\\b'\n",
        "regex_containing_digits = r'\\b\\w*\\d+\\w*\\b'\n",
        "\n",
        "\n",
        "np.set_printoptions(precision = 2, linewidth = 200)\n",
        "\n",
        "stopwords_punc = set(stopwords.words('english')).union(set(punctuation))\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "\n",
        "  # removes html escape characters\n",
        "  tweet = html.unescape(tweet)\n",
        "  # removes urls\n",
        "  tweet = re.sub(regex_url, '', tweet)\n",
        "  # replaces . or : with space\n",
        "  tweet = re.sub(regex_dots, ' ', tweet)\n",
        "  # removes spec chars and leaves the words attached to them\n",
        "  tweet = re.sub(regex_spec_chars, '', tweet)\n",
        "  # removes stock market info, prices, etc.\n",
        "  tweet = re.sub(regex_currencies, '', tweet)\n",
        "  # removes numbers, dates\n",
        "  tweet = re.sub(regex_numeric, '', tweet)\n",
        "  # removes entire words with digit or digits\n",
        "  tweet = re.sub(regex_containing_digits, '', tweet)\n",
        "\n",
        "  return tweet\n",
        "\n",
        "def tokenize_word(word):\n",
        "  return lancaster.stem(word)\n",
        "\n",
        "def tokenize_tweet(tweet):\n",
        "\n",
        "  # exclude trash input\n",
        "  tweet = clean_tweet(tweet)\n",
        "  # split tweet into an array of words\n",
        "  words = word_tokenize(tweet)\n",
        "  # all lower case\n",
        "  words_lower = [w.lower() for w in words]\n",
        "  # remove stopwords, punctuation\n",
        "  words_filtered = [tokenize_word(w) for w in words_lower if w not in stopwords_punc]\n",
        "\n",
        "  return words_filtered\n",
        "\n",
        "\n",
        "tadija_path = '/content/drive/MyDrive/ML2024_D1/disaster-tweets.csv'\n",
        "mina_path = '/content/drive/MyDrive/6003 ML/data/disaster-tweets.csv'\n",
        "\n",
        "dataset = []\n",
        "tweet_column_idx, target_column_idx = 3, 4\n",
        "sample_nb = 7614\n",
        "\n",
        "\n",
        "Y_targets = np.zeros(sample_nb, dtype=np.int64)\n",
        "\n",
        "with open(tadija_path, 'r', newline='') as csvfile:\n",
        "    csv_reader = csv.reader(csvfile)\n",
        "\n",
        "    # skip the header at 1st line\n",
        "    next(csv_reader)\n",
        "\n",
        "    for idx, row in enumerate(csv_reader):\n",
        "\n",
        "      if idx >= sample_nb:\n",
        "          break\n",
        "\n",
        "      tweet = row[tweet_column_idx]\n",
        "      dataset.append(tokenize_tweet(tweet))\n",
        "\n",
        "      Y_targets[idx] = int(row[target_column_idx])\n",
        "\n",
        "\n",
        "\n",
        "# IDF MAP\n",
        "token_count = dict()\n",
        "lr_map = dict()\n",
        "\n",
        "for idx, sample in enumerate(dataset):\n",
        "\n",
        "    for token in sample:\n",
        "\n",
        "        if token not in token_count:\n",
        "            token_count[token] = 0\n",
        "            lr_map[token] = [0, 0]\n",
        "\n",
        "\n",
        "        token_count[token] += 1\n",
        "        lr_map[token][Y_targets[idx]] += 1\n",
        "\n",
        "most_common_pos = list(dict(sorted(lr_map.items(), key=lambda item: item[1][0], reverse = True)).items())[:5]\n",
        "most_common_neg = list(dict(sorted(lr_map.items(), key=lambda item: item[1][1], reverse = True)).items())[:5]\n",
        "\n",
        "print('Five most commonly used words in positive tweets: ', most_common_pos)\n",
        "print('Five most commonly used words in negative tweets: ', most_common_neg, '\\n')\n",
        "\n",
        "\n",
        "lr_map = {key: value for key, value in lr_map.items() if value[0] > 10 and value[1] > 10}\n",
        "lr_map_sorted = dict(sorted(lr_map.items(), key=lambda item: item[1][0] / item[1][1]))\n",
        "\n",
        "first_five = dict(list(lr_map_sorted.items())[:5])\n",
        "last_five = dict(list(lr_map_sorted.items())[-5:])\n",
        "\n",
        "print('Tokens with biggest LR scores')\n",
        "\n",
        "for token, counts in first_five.items():\n",
        "  print(f'Token: \"{token}\" | positive mentions: {counts[0]} | negative mentions: {counts[1]} | LR score = {(counts[0]/counts[1]):.5f}')\n",
        "\n",
        "\n",
        "print('\\nTokens with lowest LR scores')\n",
        "\n",
        "for token, counts in last_five.items():\n",
        "  print(f'Token: \"{token}\" | positive mentions: {counts[0]} | negative mentions: {counts[1]} | LR score = {(counts[0]/counts[1]):.5f}')\n",
        "\n",
        "print('\\nThe entire LR matrix: ', lr_map, '\\n')\n",
        "\n",
        "token_count = dict(sorted(token_count.items(), key=lambda entry: entry[1], reverse=True))\n",
        "\n",
        "# feature vector dimension limit\n",
        "feat_max = 10000\n",
        "\n",
        "# if nb of unique tokens in vocab exceeds feat_max we trim the excess\n",
        "if(len(token_count) > feat_max):\n",
        "    token_count = dict(list(token_count.items())[:feat_max])\n",
        "\n",
        "count = 0\n",
        "print('Vocabulary: ')\n",
        "for token, count in token_count.items():\n",
        "  if count < 100:\n",
        "    break\n",
        "  print('token: ', token, 'count: ', count, ' |')\n",
        "  count += 1\n",
        "\n",
        "def score_token(token, sample, token_count):\n",
        "\n",
        "  inverse_doc_freq = math.log10(len(dataset)/token_count[token])\n",
        "\n",
        "  local_freq = sample.count(token) / len(sample)\n",
        "\n",
        "  # return local_freq * inverse_doc_freq\n",
        "  return 1 if token in sample else 0\n",
        "\n",
        "X_features = np.zeros((sample_nb, len(token_count)), dtype=np.float32)\n",
        "\n",
        "sample_idx, token_idx = 0, 0\n",
        "\n",
        "for sample in dataset:\n",
        "\n",
        "  token_idx = 0\n",
        "\n",
        "  for token, count in token_count.items():\n",
        "\n",
        "    token_score = score_token(token=token,sample=sample,token_count=token_count)\n",
        "    X_features[sample_idx][token_idx] = token_score\n",
        "    token_idx += 1\n",
        "\n",
        "  sample_idx += 1\n",
        "\n",
        "sum_accuracy = 0\n",
        "avg_accuracy = 0.0\n",
        "\n",
        "# spliting the data 80:20\n",
        "train_test_split = int(len(X_features) * 0.8)\n",
        "\n",
        "n = 3\n",
        "for i in range (1, n+1):\n",
        "\n",
        "  curr_accuracy = 0\n",
        "\n",
        "  # shuffling data\n",
        "  indices = np.random.permutation(len(X_features))\n",
        "\n",
        "  X_features = X_features[indices]\n",
        "  Y_targets = Y_targets[indices]\n",
        "\n",
        "  X_train = X_features[:train_test_split]\n",
        "  Y_train = Y_targets[:train_test_split]\n",
        "\n",
        "  X_test = X_features[train_test_split:]\n",
        "  Y_test = Y_targets[train_test_split:]\n",
        "\n",
        "  model = MultinomialNaiveBayes(nb_classes=2, nb_words=len(token_count), pseudocount=1)\n",
        "  model.fit(X_features, Y_targets)\n",
        "\n",
        "  hit_count = 0\n",
        "\n",
        "  for idx in range(len(X_test)):\n",
        "\n",
        "    prediction = model.predict(X_test[idx])\n",
        "    # prediction = model.predict_multiply(X_test[idx])\n",
        "\n",
        "\n",
        "    if prediction == Y_test[idx]:\n",
        "      hit_count += 1\n",
        "\n",
        "    if idx % 100 == 0:\n",
        "      print(f'Prediction for sample number: {str(idx)} / {len(X_test)} is {str(prediction)}')\n",
        "\n",
        "  curr_accuracy += hit_count / len(X_test)\n",
        "  print('Accuracy: ', str(curr_accuracy), 'for try number', str(i))\n",
        "\n",
        "  sum_accuracy += curr_accuracy\n",
        "\n",
        "avg_accuracy = sum_accuracy / n\n",
        "print('\\nAverage accuracy for all three tries: {:.5f}'.format(avg_accuracy))\n"
      ],
      "metadata": {
        "id": "53Y2k2pIOvEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "478199d2-b2f2-4728-b391-2e58d7829bfc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Five most commonly used words in positive tweets:  [('lik', [308, 102]), ('im', [262, 69]), ('new', [228, 195]), ('get', [223, 88]), ('dont', [154, 55])]\n",
            "Five most commonly used words in negative tweets:  [('fir', [92, 272]), ('bomb', [51, 242]), ('new', [228, 195]), ('kil', [27, 163]), ('via', [99, 121])] \n",
            "\n",
            "Tokens with biggest LR scores\n",
            "Token: \"kil\" | positive mentions: 27 | negative mentions: 163 | LR score = 0.16564\n",
            "Token: \"train\" | positive mentions: 19 | negative mentions: 109 | LR score = 0.17431\n",
            "Token: \"report\" | positive mentions: 14 | negative mentions: 72 | LR score = 0.19444\n",
            "Token: \"bomb\" | positive mentions: 51 | negative mentions: 242 | LR score = 0.21074\n",
            "Token: \"near\" | positive mentions: 14 | negative mentions: 63 | LR score = 0.22222\n",
            "\n",
            "Tokens with lowest LR scores\n",
            "Token: \"fuck\" | positive mentions: 77 | negative mentions: 14 | LR score = 5.50000\n",
            "Token: \"ful\" | positive mentions: 88 | negative mentions: 16 | LR score = 5.50000\n",
            "Token: \"yo\" | positive mentions: 69 | negative mentions: 11 | LR score = 6.27273\n",
            "Token: \"oblit\" | positive mentions: 79 | negative mentions: 12 | LR score = 6.58333\n",
            "Token: \"lov\" | positive mentions: 120 | negative mentions: 17 | LR score = 7.05882\n",
            "\n",
            "The entire LR matrix:  {'may': [46, 51], 'us': [142, 107], 'forest': [16, 51], 'fir': [92, 272], 'near': [14, 63], 'plac': [18, 17], 'not': [17, 12], 'off': [38, 83], 'evacu': [29, 101], 'ord': [14, 24], 'expect': [11, 22], 'peopl': [94, 106], 'got': [97, 33], 'photo': [37, 28], 'smok': [41, 14], 'school': [36, 30], 'upd': [19, 37], 'flood': [47, 120], 'disast': [39, 119], 'rain': [20, 38], 'caus': [41, 64], 'im': [262, 69], 'top': [41, 16], 'hil': [14, 11], 'see': [100, 30], 'ther': [29, 17], 'emerg': [84, 79], 'hap': [29, 29], 'build': [55, 98], 'tornado': [11, 27], 'com': [110, 46], 'heat': [17, 31], 'wav': [30, 45], 'far': [12, 16], 'get': [223, 88], 'wait': [28, 11], 'second': [17, 18], 'liv': [45, 75], 'gon': [45, 16], 'na': [62, 17], 'day': [93, 44], 'ar': [28, 23], 'dam': [28, 52], 'bus': [11, 37], 'car': [88, 87], 'crash': [51, 114], 'break': [28, 30], 'man': [83, 54], 'lov': [120, 17], 'sum': [28, 17], 'cool': [25, 11], 'way': [61, 27], 'cant': [91, 30], 'shit': [38, 19], 'last': [57, 30], 'week': [32, 11], 'lik': [308, 102], 'past': [17, 22], 'end': [40, 23], 'market': [32, 12], 'ablaz': [16, 12], 'rt': [60, 47], 'new': [228, 195], 'set': [30, 33], 'ab': [11, 12], 'look': [90, 54], 'night': [41, 11], 'much': [51, 13], 'around': [23, 17], 'two': [27, 72], 'head': [40, 20], 'st': [11, 19], 'pol': [41, 110], 'outsid': [13, 12], 'yo': [69, 11], 'al': [15, 12], 'dead': [35, 70], 'insid': [13, 16], 'tim': [104, 59], 'sit': [24, 30], 'thank': [49, 18], 'tak': [78, 49], 'want': [99, 19], 'follow': [29, 22], 'know': [91, 31], 'stat': [34, 50], 'burn': [94, 85], 'lif': [60, 27], 'leav': [23, 21], 'first': [59, 49], 'every': [44, 15], 'next': [34, 17], 'year': [64, 84], 'shot': [11, 31], 'hom': [40, 103], 'arson': [22, 28], 'black': [48, 21], 'train': [19, 109], 'gun': [18, 13], 'lat': [14, 13], 'oth': [11, 12], 'truck': [20, 35], 'heart': [28, 11], 'city': [21, 47], 'tonight': [25, 18], 'video': [102, 72], 'would': [98, 34], 'fal': [45, 28], 'hug': [12, 13], 'talk': [21, 12], 'go': [67, 29], 'dont': [154, 55], 'mak': [100, 34], 'work': [76, 33], 'accid': [28, 77], 'w': [33, 14], 'mov': [24, 27], 'cent': [20, 38], 'block': [18, 12], 'gre': [49, 16], 'read': [59, 19], 'help': [55, 34], 'report': [14, 72], 'pleas': [48, 27], 'mil': [16, 31], 'n': [12, 16], 'mom': [34, 13], 'didnt': [21, 11], 'horr': [20, 16], 'fin': [59, 17], 'god': [50, 15], 'tel': [22, 14], 'anoth': [38, 33], 'die': [21, 20], 'med': [18, 37], 'turn': [30, 11], 'everyon': [42, 16], 'stop': [41, 26], 'back': [86, 36], 'min': [53, 29], 'injury': [55, 32], 'right': [51, 25], 'chang': [44, 18], 'support': [16, 14], 'plan': [58, 73], 'tre': [18, 24], 'today': [47, 55], 'il': [30, 11], 'ev': [106, 57], 'fuck': [77, 14], 'driv': [31, 31], 'road': [18, 28], 'kil': [27, 163], 'explod': [75, 46], 'stil': [73, 57], 'heard': [24, 11], 'lead': [25, 18], 'issu': [13, 46], 'gam': [42, 11], 'ban': [24, 11], 'ir': [18, 36], 'rememb': [26, 14], 'cam': [25, 15], 'also': [33, 11], 'going': [75, 28], 'on': [136, 70], 'act': [57, 53], 'fre': [33, 11], 'best': [54, 21], 'minut': [11, 30], 'could': [44, 38], 'real': [99, 37], 'many': [51, 33], 'murd': [23, 65], 'ful': [88, 16], 'youtub': [76, 22], 'fac': [39, 16], 'thing': [54, 19], 'believ': [23, 13], 'poss': [11, 28], 'trap': [31, 16], 'think': [82, 23], 'nev': [44, 17], 'say': [76, 67], 'don': [19, 16], 'nee': [72, 29], 'play': [60, 14], 'begin': [14, 14], 'good': [68, 23], 'famy': [27, 105], 'via': [99, 121], 'men': [19, 13], 'govern': [17, 31], 'almost': [14, 11], 'mod': [28, 19], 'wreck': [84, 57], 'ins': [21, 18], 'nat': [44, 58], 'dea': [37, 36], 'ago': [11, 16], 'littl': [31, 18], 'traum': [52, 22], 'ship': [24, 14], 'cop': [15, 13], 'hous': [27, 43], 'ear': [34, 20], 'wak': [14, 16], 'cal': [38, 54], 'amb': [19, 24], 'fear': [45, 45], 'sery': [21, 16], 'serv': [53, 45], 'destroy': [60, 37], 'blood': [33, 11], 'crazy': [11, 11], 'fight': [23, 29], 'run': [48, 36], 'ok': [20, 22], 'hour': [25, 15], 'park': [15, 14], 'said': [31, 26], 'respond': [25, 26], 'sir': [45, 19], 'numb': [13, 11], 'body': [117, 35], 'hat': [40, 13], 'annihil': [36, 19], 'show': [49, 19], 'saf': [16, 16], 'hit': [31, 34], 'must': [17, 17], 'wel': [37, 15], 'that': [38, 24], 'sint': [24, 30], 'hist': [11, 20], 'whol': [25, 16], 'hum': [14, 18], 'went': [21, 13], 'weath': [12, 40], 'surv': [68, 62], 'leg': [11, 12], 'form': [12, 15], 'army': [30, 42], 'food': [21, 27], 'hel': [21, 12], 'tot': [43, 14], 'destruct': [28, 13], 'pot': [11, 20], 'riv': [17, 17], 'u': [71, 58], 'wild': [15, 33], 'world': [72, 45], 'attack': [49, 106], 'sign': [25, 33], 'sav': [39, 42], 'without': [38, 13], 'away': [26, 12], 'feel': [77, 16], 'boy': [17, 35], 'child': [15, 12], 'watch': [58, 61], 'august': [11, 25], 'red': [25, 13], 'hot': [46, 21], 'start': [47, 18], 'high': [29, 21], 'movy': [34, 19], 'storm': [33, 93], 'latest': [12, 48], 'giv': [25, 15], 'post': [31, 28], 'peac': [16, 16], 'girl': [40, 12], 'id': [27, 14], 'long': [47, 21], 'dat': [36, 15], 'tomorrow': [23, 12], 'let': [94, 18], 'prep': [18, 17], 'saw': [15, 21], 'hail': [17, 27], 'sad': [11, 12], 'unit': [14, 22], 'part': [22, 19], 'collaps': [47, 69], 'light': [34, 14], 'pick': [26, 28], 'fan': [34, 11], 'host': [30, 49], 'wom': [64, 32], 'ind': [22, 89], 'hop': [46, 22], 'milit': [35, 39], 'war': [42, 67], 'learn': [15, 15], 'viol': [14, 33], 'control': [15, 15], 'ter': [20, 82], 'charg': [13, 44], 'tru': [20, 22], 'link': [19, 17], 'sound': [36, 14], 'fail': [21, 35], 'whit': [25, 13], 'am': [11, 26], 'busy': [30, 13], 'big': [30, 35], 'story': [26, 25], 'miss': [26, 48], 'stay': [24, 11], 'nuclear': [31, 71], 'bomb': [51, 242], 'blast': [18, 18], 'weapon': [48, 40], 'releas': [26, 34], 'fat': [60, 94], 'group': [12, 26], 'person': [25, 16], 'mad': [45, 17], 'flam': [32, 29], 'cre': [28, 12], 'deal': [22, 13], 'pow': [19, 25], 'battl': [32, 15], 'spac': [19, 13], 'intern': [22, 13], 'germ': [12, 13], 'mass': [29, 53], 'fedex': [14, 21], 'transport': [11, 18], 'bioter': [21, 36], 'lab': [11, 21], 'cut': [13, 13], 'hold': [15, 13], 'hear': [21, 14], 'keep': [36, 15], 'heal': [21, 12], 'sec': [13, 23], 'spec': [21, 19], 'pic': [13, 22], 'old': [23, 40], 'p': [24, 16], 'send': [26, 13], 'walk': [20, 13], 'wind': [18, 36], 'level': [17, 13], 'mat': [15, 13], 'land': [14, 41], 'op': [46, 21], 'refug': [13, 34], 'tragedy': [13, 23], 'rock': [12, 12], 'wound': [38, 48], 'larg': [11, 14], 'drown': [72, 29], 'par': [18, 12], 'lightn': [20, 24], 'effect': [23, 15], 'oil': [11, 44], 'sink': [36, 12], 'half': [17, 16], 'paty': [11, 13], 'dang': [34, 13], 'baby': [28, 14], 'islam': [17, 24], 'hur': [14, 28], 'nearby': [14, 13], 'react': [17, 16], 'rescu': [43, 77], 'riot': [38, 42], 'hazard': [43, 31], 'camp': [15, 16], 'mem': [14, 28], 'wat': [16, 49], 'catastroph': [37, 34], 'casual': [20, 41], 'ris': [18, 19], 'derail': [26, 88], 'chem': [21, 16], 'landslid': [17, 22], 'cost': [12, 32], 'collid': [53, 54], 'devast': [20, 36], 'delug': [50, 13], 'demol': [48, 15], 'desol': [45, 11], 'deton': [47, 50], 'loud': [34, 11], 'dust': [13, 25], 'thund': [25, 21], 'seism': [13, 14], 'tsunam': [18, 13], 'electrocut': [48, 15], 'engulf': [17, 20], 'eyewit': [18, 13], 'famin': [12, 22], 'hijack': [44, 49], 'prebreak': [17, 13], 'sinkhol': [14, 27], 'mudslid': [25, 12], 'oblit': [79, 12], 'quarantin': [52, 21], 'structural': [12, 25], 'snowstorm': [15, 12], 'whirlwind': [23, 14], 'windstorm': [23, 14]} \n",
            "\n",
            "Vocabulary: \n",
            "token:  new count:  423  |\n",
            "token:  lik count:  410  |\n",
            "token:  fir count:  364  |\n",
            "token:  im count:  331  |\n",
            "token:  get count:  311  |\n",
            "token:  bomb count:  293  |\n",
            "token:  us count:  249  |\n",
            "token:  via count:  220  |\n",
            "token:  dont count:  209  |\n",
            "token:  on count:  206  |\n",
            "token:  peopl count:  200  |\n",
            "token:  kil count:  190  |\n",
            "token:  burn count:  179  |\n",
            "token:  car count:  175  |\n",
            "token:  video count:  174  |\n",
            "token:  flood count:  167  |\n",
            "token:  crash count:  165  |\n",
            "token:  emerg count:  163  |\n",
            "token:  tim count:  163  |\n",
            "token:  ev count:  163  |\n",
            "token:  disast count:  158  |\n",
            "token:  com count:  156  |\n",
            "token:  attack count:  155  |\n",
            "token:  fat count:  154  |\n",
            "token:  build count:  153  |\n",
            "token:  body count:  152  |\n",
            "token:  pol count:  151  |\n",
            "token:  year count:  148  |\n",
            "token:  look count:  144  |\n",
            "token:  hom count:  143  |\n",
            "token:  say count:  143  |\n",
            "token:  wreck count:  141  |\n",
            "token:  day count:  137  |\n",
            "token:  man count:  137  |\n",
            "token:  lov count:  137  |\n",
            "token:  real count:  136  |\n",
            "token:  mak count:  134  |\n",
            "token:  would count:  132  |\n",
            "token:  famy count:  132  |\n",
            "token:  plan count:  131  |\n",
            "token:  evacu count:  130  |\n",
            "token:  got count:  130  |\n",
            "token:  see count:  130  |\n",
            "token:  stil count:  130  |\n",
            "token:  surv count:  130  |\n",
            "token:  u count:  129  |\n",
            "token:  train count:  128  |\n",
            "token:  tak count:  127  |\n",
            "token:  storm count:  126  |\n",
            "token:  know count:  122  |\n",
            "token:  back count:  122  |\n",
            "token:  off count:  121  |\n",
            "token:  cant count:  121  |\n",
            "token:  explod count:  121  |\n",
            "token:  liv count:  120  |\n",
            "token:  rescu count:  120  |\n",
            "token:  californ count:  119  |\n",
            "token:  watch count:  119  |\n",
            "token:  want count:  118  |\n",
            "token:  bag count:  118  |\n",
            "token:  world count:  117  |\n",
            "token:  suicid count:  117  |\n",
            "token:  collaps count:  116  |\n",
            "token:  derail count:  114  |\n",
            "token:  let count:  112  |\n",
            "token:  ind count:  111  |\n",
            "token:  act count:  110  |\n",
            "token:  work count:  109  |\n",
            "token:  war count:  109  |\n",
            "token:  first count:  108  |\n",
            "token:  rt count:  107  |\n",
            "token:  collid count:  107  |\n",
            "token:  caus count:  105  |\n",
            "token:  dead count:  105  |\n",
            "token:  accid count:  105  |\n",
            "token:  think count:  105  |\n",
            "token:  ful count:  104  |\n",
            "token:  going count:  103  |\n",
            "token:  today count:  102  |\n",
            "token:  nat count:  102  |\n",
            "token:  ter count:  102  |\n",
            "token:  nuclear count:  102  |\n",
            "token:  nee count:  101  |\n",
            "token:  drown count:  101  |\n",
            "\n",
            "Priors:\n",
            "[0.57 0.43]\n",
            "\n",
            "Occurences:\n",
            "[[220. 294.  87. ...   1.   1.   1.]\n",
            " [186.  98. 247. ...   0.   0.   0.]]\n",
            "\n",
            "Likelihoods:\n",
            "[[4.87e-03 6.50e-03 1.94e-03 ... 4.41e-05 4.41e-05 4.41e-05]\n",
            " [4.81e-03 2.55e-03 6.38e-03 ... 2.57e-05 2.57e-05 2.57e-05]]\n",
            "\n",
            "Prediction for sample number: 0 / 1523 is 0\n",
            "Prediction for sample number: 100 / 1523 is 1\n",
            "Prediction for sample number: 200 / 1523 is 0\n",
            "Prediction for sample number: 300 / 1523 is 0\n",
            "Prediction for sample number: 400 / 1523 is 0\n",
            "Prediction for sample number: 500 / 1523 is 1\n",
            "Prediction for sample number: 600 / 1523 is 0\n",
            "Prediction for sample number: 700 / 1523 is 0\n",
            "Prediction for sample number: 800 / 1523 is 1\n",
            "Prediction for sample number: 900 / 1523 is 1\n",
            "Prediction for sample number: 1000 / 1523 is 0\n",
            "Prediction for sample number: 1100 / 1523 is 0\n",
            "Prediction for sample number: 1200 / 1523 is 0\n",
            "Prediction for sample number: 1300 / 1523 is 1\n",
            "Prediction for sample number: 1400 / 1523 is 0\n",
            "Prediction for sample number: 1500 / 1523 is 0\n",
            "Accuracy:  0.8850952068286277 for try number 1\n",
            "\n",
            "Priors:\n",
            "[0.57 0.43]\n",
            "\n",
            "Occurences:\n",
            "[[220. 294.  87. ...   1.   1.   1.]\n",
            " [186.  98. 247. ...   0.   0.   0.]]\n",
            "\n",
            "Likelihoods:\n",
            "[[4.87e-03 6.50e-03 1.94e-03 ... 4.41e-05 4.41e-05 4.41e-05]\n",
            " [4.81e-03 2.55e-03 6.38e-03 ... 2.57e-05 2.57e-05 2.57e-05]]\n",
            "\n",
            "Prediction for sample number: 0 / 1523 is 0\n",
            "Prediction for sample number: 100 / 1523 is 1\n",
            "Prediction for sample number: 200 / 1523 is 0\n",
            "Prediction for sample number: 300 / 1523 is 0\n",
            "Prediction for sample number: 400 / 1523 is 1\n",
            "Prediction for sample number: 500 / 1523 is 0\n",
            "Prediction for sample number: 600 / 1523 is 1\n",
            "Prediction for sample number: 700 / 1523 is 1\n",
            "Prediction for sample number: 800 / 1523 is 0\n",
            "Prediction for sample number: 900 / 1523 is 1\n",
            "Prediction for sample number: 1000 / 1523 is 0\n",
            "Prediction for sample number: 1100 / 1523 is 0\n",
            "Prediction for sample number: 1200 / 1523 is 1\n",
            "Prediction for sample number: 1300 / 1523 is 0\n",
            "Prediction for sample number: 1400 / 1523 is 1\n",
            "Prediction for sample number: 1500 / 1523 is 0\n",
            "Accuracy:  0.8719632304661852 for try number 2\n",
            "\n",
            "Priors:\n",
            "[0.57 0.43]\n",
            "\n",
            "Occurences:\n",
            "[[220. 294.  87. ...   1.   1.   1.]\n",
            " [186.  98. 247. ...   0.   0.   0.]]\n",
            "\n",
            "Likelihoods:\n",
            "[[4.87e-03 6.50e-03 1.94e-03 ... 4.41e-05 4.41e-05 4.41e-05]\n",
            " [4.81e-03 2.55e-03 6.38e-03 ... 2.57e-05 2.57e-05 2.57e-05]]\n",
            "\n",
            "Prediction for sample number: 0 / 1523 is 0\n",
            "Prediction for sample number: 100 / 1523 is 0\n",
            "Prediction for sample number: 200 / 1523 is 0\n",
            "Prediction for sample number: 300 / 1523 is 0\n",
            "Prediction for sample number: 400 / 1523 is 1\n",
            "Prediction for sample number: 500 / 1523 is 0\n",
            "Prediction for sample number: 600 / 1523 is 1\n",
            "Prediction for sample number: 700 / 1523 is 1\n",
            "Prediction for sample number: 800 / 1523 is 0\n",
            "Prediction for sample number: 900 / 1523 is 0\n",
            "Prediction for sample number: 1000 / 1523 is 1\n",
            "Prediction for sample number: 1100 / 1523 is 0\n",
            "Prediction for sample number: 1200 / 1523 is 0\n",
            "Prediction for sample number: 1300 / 1523 is 1\n",
            "Prediction for sample number: 1400 / 1523 is 0\n",
            "Prediction for sample number: 1500 / 1523 is 1\n",
            "Accuracy:  0.8765594221930401 for try number 3\n",
            "\n",
            "Average accuracy for all three tries: 0.87787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mksuub0xjXMO",
        "outputId": "077cb198-8c00-40e5-fa77-f86b87f30cf5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrika positive/negative daje veci broj sto je broj koriscenja reci u pozitivnim tvitovima veci u odnosu na broj koriscenja iste reci u negativnim tvitovima, a sto je veci broj koriscenja reci u negativnim nego u pozitivnim tvitovima, to je vrednost metrike bliza nuli. Metrika primenjena na konkretnu rec moze nam sluziti kao vid aproksimacije verovatnoce da je tvit koji sadrzi ovu rec pozitivan ili negativan.\n",
        "\n",
        "Na osnovu broja pojavljivanja ove reci metrika LR br_poz_koriscenja/br_neg_koriscenja moze predstavljati \"tezinu\" uticaja konkretne reci na to da li je tvit pozitivan ili negativan. Kao i Naive Bayes model, ova metrika moze dati neku aproksimaciju na osnovu samog broja koriscenja svake reci u tvitovima bez uzimanja konteksta koriscenja reci u obzir."
      ],
      "metadata": {
        "id": "BVXSS5okY9Ky"
      }
    }
  ]
}